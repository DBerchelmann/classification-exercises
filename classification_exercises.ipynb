{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In a jupyter notebook, classification_exercises.ipynb, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, df_iris, from this data.\n",
    "\n",
    "- print the first 3 rows\n",
    "- print the number of rows and columns (shape)\n",
    "- print the column names\n",
    "- print the data type of each column\n",
    "- print the summary statistics for each of the numeric variables. Would you recommend rescaling the data based on       these statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# visualize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(11, 9))\n",
    "plt.rc('font', size=13)\n",
    "\n",
    "# turn off pink warning boxes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# acquire\n",
    "from env import host, user, password\n",
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data('iris', show_doc=False)\n",
    "\n",
    "df_iris = data('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print first 3 rows\n",
    "\n",
    "print(df_iris.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows and columns\n",
    "\n",
    "print(df_iris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names\n",
    "\n",
    "print(df_iris.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data type of each column\n",
    "\n",
    "print(df_iris.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary statistics for each of the numeric variables. \n",
    "# Would you recommend rescaling the data based on these statistics?\n",
    "\n",
    "df_iris[[\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"]].describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df_iris.describe().T\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['range'] = stats['max']-stats['min']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read the Table1_CustDetails table from the Excel_Exercises.xlsx file into a dataframe named df_excel.\n",
    "\n",
    "- assign the first 100 rows to a new dataframe, df_excel_sample\n",
    "- print the number of rows of your original dataframe\n",
    "- print the first 5 column names\n",
    "- print the column names that have a data type of object\n",
    "- compute the range for each of the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel = pd.read_excel('excel_exercises.xlsx', sheet_name='Table1_CustDetails')\n",
    "df_excel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the first 100 rows to a new dataframe, df_excel_sample\n",
    "\n",
    "df_excel_sample = df_excel.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows of your original dataframe\n",
    "\n",
    "excel_index = df_excel.index\n",
    "\n",
    "excel_rows = len(excel_index)\n",
    "\n",
    "print(f'The number of rows in the excel data frame is {excel_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 column names\n",
    "\n",
    "print(df_excel[['customer_id', 'gender', 'is_senior_citizen', 'partner', 'dependents']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names that have a data type of object\n",
    "\n",
    "df_excel.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_objects = df_excel.select_dtypes(include='object').columns.to_list()\n",
    "\n",
    "print(excel_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_excel[['customer_id', 'gender', 'partner', 'dependents', 'payment_type', 'churn']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the range for each of the numeric variables\n",
    "df_excel.select_dtypes(include='float64').columns\n",
    "\n",
    "(df_excel.select_dtypes(include='float64').max()) - (df_excel.select_dtypes(include='float64').min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the range for each of the numeric variables\n",
    "\n",
    "excel_numeric_range = (df_excel[['monthly_charges', 'total_charges']].max()) - (df_excel[['monthly_charges', 'total_charges']].min())\n",
    "excel_numeric_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read the data from this google sheet into a dataframe, df_google\n",
    "\n",
    "- print the first 3 rows\n",
    "- print the number of rows and columns\n",
    "- print the column names\n",
    "- print the data type of each column\n",
    "- print the summary statistics for each of the numeric variables\n",
    "- print the unique values for each of your categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the replace method to modify our Google Sheet share url to be a csv export url.\n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use read_csv() method to create our DataFrame.\n",
    "\n",
    "df_google = pd.read_csv(csv_export_url)\n",
    "\n",
    "type(df_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 3 rows\n",
    "\n",
    "df_google.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows and columns\n",
    "\n",
    "print(df_google.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names\n",
    "\n",
    "print(df_google.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data type of each column\n",
    "\n",
    "print(df_google.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary statistics for each of the numeric variables\n",
    "\n",
    "df_google[['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.describe(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the unique values for each of your categorical variables\n",
    "\n",
    "df_google.Name.unique()\n",
    "df_google.Sex.unique()\n",
    "df_google.Ticket.unique()\n",
    "df_google.Cabin.unique()\n",
    "df_google.Embarked.unique()\n",
    "\n",
    "\n",
    "# df = df_googlesheet.select_dtypes(exclude=['int', 'float'])\n",
    "# for col in df_columns:\n",
    "#    print(df_[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the best way to count the unique values for each column name that has a data type as 'object'\n",
    "\n",
    "for col in df_google:\n",
    "    if df_google[col].dtypes == 'object':\n",
    "        print(f'{col} has {df_google[col].nunique()} unique values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_google.select_dtypes('object'):\n",
    "    print(f'{col} has {df_google[col].nunique()} unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.Survived.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.Embarked.value_counts(dropna=False)# this allows for us to see how many NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google[['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a function named get_titanic_data that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the Codeup Data Science Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of my functions are in my acquire.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_titanic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic = get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_iris = get_iris_data()\n",
    "\n",
    "new_df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The end product of this exercise should be the specified functions in a python script named prepare.py. Do these in your classification_exercises.ipynb first, then transfer to the prepare.py file.\n",
    "\n",
    " - This work should all be saved in your local classification-exercises repo. Then add, commit, and push your changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use the function defined in acquire.py to load the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_prac = acquire.get_iris_data()\n",
    "\n",
    "iris_prac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = iris_prac.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe how many columns have any missing values\n",
    "missing[missing > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_prac.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop the species_id and measurement_id columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris_prac = iris_prac.drop(['species_id', 'measurement_id'], axis=1)\n",
    "\n",
    "# iris_prac.drop(columns=['species_id', 'measurement_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = ['species_id', 'measurement_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_prac.drop(columns=dropcols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rename the species_name column to just species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns={'species_name': 'species'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_prac['species'] = iris_prac.species_name\n",
    "\n",
    "iris_prac.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create dummy variables of the species name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_prac.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(iris_prac[['species']], drop_first=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_prac = pd.concat([iris_prac, dummies], axis=1)\n",
    "\n",
    "iris_prac.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_iris():\n",
    "    '''\n",
    "    prep_iris will take a dataframe acquired as df and remove species_id and \n",
    "    measurement_id. The function will then rename the species_name col to 'species'\n",
    "    Finally, the categorical species name will have dummy values created for them and the\n",
    "    table will be concatanted to bring it all together as on dataframe\n",
    "    \n",
    "    return: single cleaned dataframe\n",
    "    '''\n",
    "    \n",
    "    dropcols = ['species_id', 'measurement_id']\n",
    "    iris_prac.drop(columns=dropcols, inplace=True)\n",
    "    iris_prac['species'] = iris_prac.species_name\n",
    "    dummies = pd.get_dummies(iris_prac[['species']], drop_first=False)\n",
    "    pd.get_dummies(iris_prac[['species']], drop_first=False)\n",
    "    return pd.concat([iris_prac, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Train, Test, Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(iris_prac, test_size=.2, random_state=123, stratify=df.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=None, strategy = 'most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = imputer.fit(train[['species']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mode():\n",
    "    '''\n",
    "    impute mode for species\n",
    "    '''\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    train[['species']] = imputer.fit_transform(train[['species']])\n",
    "    validate[['species']] = imputer.transform(validate[['species']])\n",
    "    test[['species']] = imputer.transform(test[['species']])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_iris_data():\n",
    "    df = clean_iris()\n",
    "    train_validate, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.species)\n",
    "    train, validate = train_test_split(train_validate, \n",
    "                                       test_size=.3, \n",
    "                                       random_state=123, \n",
    "                                       stratify=train_validate.species)\n",
    "    train, validate, test = impute_mode()\n",
    "    return train, validate, test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
